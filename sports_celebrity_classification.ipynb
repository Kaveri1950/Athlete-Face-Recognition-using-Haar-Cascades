{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "1eHf6ohNT3cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pywt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn import svm\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DbsbziJXrfFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Haar Cascade Classifiers"
      ],
      "metadata": {
        "id": "8L7iNDi9T_jf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cchuv8oitz1d"
      },
      "outputs": [],
      "source": [
        "face_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/opencv/haarcascades/haarcascade_frontalface_default.xml')\n",
        "eye_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/opencv/haarcascades/haarcascade_eye.xml')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wavelet Transform Function"
      ],
      "metadata": {
        "id": "kAVlnriWUJcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lBNZR2Kztz1d"
      },
      "outputs": [],
      "source": [
        "def w2d(img, mode='haar', level=1):\n",
        "    imArray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    imArray = np.float32(imArray)\n",
        "    imArray /= 255\n",
        "    coeffs = pywt.wavedec2(imArray, mode, level=level)\n",
        "    coeffs_H = list(coeffs)\n",
        "    coeffs_H[0] *= 0\n",
        "    imArray_H = pywt.waverec2(coeffs_H, mode)\n",
        "    imArray_H *= 255\n",
        "    imArray_H = np.uint8(imArray_H)\n",
        "    return imArray_H"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face and Eye Detector"
      ],
      "metadata": {
        "id": "o3CRnn1lUOOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t5H6oBG5tz1f"
      },
      "outputs": [],
      "source": [
        "def get_cropped_image_if_2_eyes(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        return None\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        roi_color = img[y:y + h, x:x + w]\n",
        "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "        if len(eyes) >= 2:\n",
        "            return roi_color\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crop and Save Valid Face Images\n"
      ],
      "metadata": {
        "id": "Z3TxRARWUYzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ep1UHXfHtz1h"
      },
      "outputs": [],
      "source": [
        "path_to_data = \"/content/drive/MyDrive/celebrity_dataset_final\"\n",
        "path_to_cr_data = os.path.join(path_to_data, \"cropped\")\n",
        "\n",
        "#Collect image directories\n",
        "img_dirs = [entry.path for entry in os.scandir(path_to_data) if entry.is_dir()]\n",
        "\n",
        "#Clear cropped directory if it exists\n",
        "import shutil\n",
        "if os.path.exists(path_to_cr_data):\n",
        "    shutil.rmtree(path_to_cr_data)\n",
        "os.makedirs(path_to_cr_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tzaBvEKrtz1i",
        "outputId": "18f9c77e-d83b-4509-feae-6f66ff83e48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Allyson_Felix_athletics\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Anthony_Davis_basketball\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Antoine_Griezmann_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Aryna_Sabalenka_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Babar_Azam_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Ben_Stokes_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Carlos_Alcaraz_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/chris evans - Google Search\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Carl_Lewis_athletics\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/chris hemsworth - Google Search\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Coco_Gauff_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Cristiano_Ronaldo_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Daniil_Medvedev_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Elaine_Thompson-Herah_athletics\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Florence_Griffith-Joyner_athletics\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Hardik_Pandya_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Harry_Kane_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Erling_Haaland_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Giannis_Antetokounmpo_basketball\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Iga_SÃÅwiaÃ®tek_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/James_Harden_basketball\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Jannik_Sinner_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Jayson_Tatum_basketball\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Jasprit_Bumrah_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/jennifer lawrence - Google Search\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Joe_Root_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Kane_Williamson_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Karim_Benzema_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Kevin_Durant_basketball\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Kylian_MbappeÃÅ_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/KL_Rahul_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/LeBron_James_basketball\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Kyrie_Irving_basketball\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Lionel_Messi_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Luka_ModricÃÅ_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Mohamed_Salah_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Mondo_Duplantis_athletics\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/MS_Dhoni_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Naomi_Osaka_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Neeraj_Chopra_athletics\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Novak_Djokovic_tennis\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Paulo_Dybala_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Neymar_Jr_football\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Rishabh_Pant_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/robert downey jr - Google Search\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Rohit_Sharma_cricket\n",
            " Generating cropped images in folder: /content/drive/MyDrive/celebrity_dataset_final/cropped/Rafael_Nadal_tennis\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2080640588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mroi_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cropped_image_if_2_eyes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroi_color\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1858058083.py\u001b[0m in \u001b[0;36mget_cropped_image_if_2_eyes\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Face and Eye Detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_cropped_image_if_2_eyes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cropped_image_dirs = []\n",
        "celebrity_file_names_dict = {}\n",
        "\n",
        "for img_dir in img_dirs:\n",
        "    count = 1\n",
        "    celebrity_name = os.path.basename(img_dir)\n",
        "    celebrity_file_names_dict[celebrity_name] = []\n",
        "\n",
        "    for entry in os.scandir(img_dir):\n",
        "        if entry.is_file() and entry.name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            roi_color = get_cropped_image_if_2_eyes(entry.path)\n",
        "\n",
        "            if roi_color is not None:\n",
        "                cropped_folder = os.path.join(path_to_cr_data, celebrity_name)\n",
        "\n",
        "                if not os.path.exists(cropped_folder):\n",
        "                    os.makedirs(cropped_folder)\n",
        "                    cropped_image_dirs.append(cropped_folder)\n",
        "                    print(\" Generating cropped images in folder:\", cropped_folder)\n",
        "\n",
        "                cropped_file_name = f\"{celebrity_name}{count}.png\"\n",
        "                cropped_file_path = os.path.join(cropped_folder, cropped_file_name)\n",
        "\n",
        "                cv2.imwrite(cropped_file_path, roi_color)\n",
        "                celebrity_file_names_dict[celebrity_name].append(cropped_file_path)\n",
        "                count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIRVj8R_tz1k"
      },
      "outputs": [],
      "source": [
        "#Check loaded file lists from cropped folders\n",
        "print(\"\\n Verifying files per celebrity:\")\n",
        "for celeb, files in celebrity_file_names_dict.items():\n",
        "    print(f\"{celeb}: {len(files)} files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0ZTcTpztz1l"
      },
      "outputs": [],
      "source": [
        "# Filter out classes with fewer than N(min_images) images\n",
        "min_images = 40\n",
        "filtered_celebrity_file_names_dict = {\n",
        "    celeb: files\n",
        "    for celeb, files in celebrity_file_names_dict.items()\n",
        "    if len(files) >= min_images\n",
        "}\n",
        "\n",
        "print(f\"\\nAfter filtering, {len(filtered_celebrity_file_names_dict)} classes kept out of {len(celebrity_file_names_dict)}\")\n",
        "\n",
        "# Update downstream variables\n",
        "celebrity_file_names_dict = filtered_celebrity_file_names_dict\n",
        "class_dict = {name: idx for idx, name in enumerate(celebrity_file_names_dict.keys())}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X-8o7NLtz1m"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Feature Matrix (X) and Labels (y)\n",
        "X, y = [], []\n",
        "for celebrity_name, training_files in celebrity_file_names_dict.items():\n",
        "    for training_image in training_files:\n",
        "        img = cv2.imread(training_image)\n",
        "        scalled_raw_img = cv2.resize(img, (32, 32))\n",
        "        img_har = w2d(img, 'db1', 5)\n",
        "        scalled_img_har = cv2.resize(img_har, (32, 32))\n",
        "        combined_img = np.vstack((\n",
        "            scalled_raw_img.reshape(32 * 32 * 3, 1),\n",
        "            scalled_img_har.reshape(32 * 32, 1)\n",
        "        ))\n",
        "        X.append(combined_img)\n",
        "        y.append(class_dict[celebrity_name])\n",
        "\n",
        "X = np.array(X).reshape(len(X), 4096).astype(float)\n",
        "y = np.array(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-bWX2IItz1n"
      },
      "outputs": [],
      "source": [
        "# Train/Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test different Models"
      ],
      "metadata": {
        "id": "DudhC21DUeec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV for Best Model"
      ],
      "metadata": {
        "id": "2_Ef5z04UrSO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSwZH53htz1o"
      },
      "outputs": [],
      "source": [
        "model_params = {\n",
        "    'svm': {\n",
        "        'model': svm.SVC(gamma='auto', probability=True),\n",
        "        'params': {\n",
        "            'svc__C': [1, 10, 100],\n",
        "            'svc__kernel': ['rbf', 'linear']\n",
        "        }\n",
        "    },\n",
        "    'random_forest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'randomforestclassifier__n_estimators': [5, 10]\n",
        "        }\n",
        "    },\n",
        "    'logistic_regression': {\n",
        "        'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n",
        "        'params': {\n",
        "            'logisticregression__C': [1, 5, 10]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoV0LtIYtz1q",
        "outputId": "543ee053-38a8-4cbc-e3a7-256bec6530d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model Comparison:\n",
            "                  model  best_score  \\\n",
            "0                  svm    0.801667   \n",
            "1        random_forest    0.686333   \n",
            "2  logistic_regression    0.777333   \n",
            "\n",
            "                                    best_params  \n",
            "0        {'svc__C': 1, 'svc__kernel': 'linear'}  \n",
            "1  {'randomforestclassifier__n_estimators': 10}  \n",
            "2                  {'logisticregression__C': 1}  \n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "best_estimators = {}\n",
        "\n",
        "for algo, mp in model_params.items():\n",
        "    pipe = make_pipeline(StandardScaler(), mp['model'])\n",
        "    clf = GridSearchCV(pipe, mp['params'], cv=5, return_train_score=False)\n",
        "    clf.fit(X_train, y_train)\n",
        "    scores.append({\n",
        "        'model': algo,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "    best_estimators[algo] = clf.best_estimator_\n",
        "\n",
        "df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
        "print(\"\\n Model Comparison:\\n\", df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV for KNN"
      ],
      "metadata": {
        "id": "YRZcgFugU0_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'knn__n_neighbors': [3, 4, 5,6, 7, 8, 9, 11, 12, 13, 14, 15],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__p': [1, 2]  # 1=Manhattan, 2=Euclidean\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    knn_pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Cross Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "#FINAL EVALUATION\n",
        "\n",
        "best_knn = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_knn.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n Test Accuracy: {acc*100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3GmI6fsPVir",
        "outputId": "862bbb0b-faac-4762-b803-88a5d3478e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Searching for best KNN parameters...\n",
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "\n",
            "üèÜ Best Parameters: {'knn__n_neighbors': 3, 'knn__p': 2, 'knn__weights': 'distance'}\n",
            "üìà Best CV Accuracy: 0.7020\n",
            "\n",
            "üéØ Test Accuracy: 75.61%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.71      0.74        14\n",
            "           1       0.76      1.00      0.87        13\n",
            "           2       0.73      0.57      0.64        14\n",
            "\n",
            "    accuracy                           0.76        41\n",
            "   macro avg       0.75      0.76      0.75        41\n",
            "weighted avg       0.75      0.76      0.75        41\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBOOST Classifier"
      ],
      "metadata": {
        "id": "ejDu9F8VVD9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# EVALUATE\n",
        "\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n Accuracy: {acc*100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byh0L5TdA5S8",
        "outputId": "e1493264-9fc9-4fe1-eb1a-3c1cb8a7c30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [03:55:31] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Accuracy: 87.80%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90        14\n",
            "           1       0.87      1.00      0.93        13\n",
            "           2       0.91      0.71      0.80        14\n",
            "\n",
            "    accuracy                           0.88        41\n",
            "   macro avg       0.88      0.88      0.88        41\n",
            "weighted avg       0.88      0.88      0.87        41\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stack Classifier"
      ],
      "metadata": {
        "id": "-svn7Bp9W0ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = [\n",
        "    ('svm', SVC(kernel='rbf', probability=True, C=5)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=150, random_state=42)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5, weights='distance')),\n",
        "    ('xgb', XGBClassifier(\n",
        "        n_estimators=150,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=4,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss'\n",
        "    ))\n",
        "]\n",
        "\n",
        "meta_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# STACKING CLASSIFIER PIPELINE\n",
        "\n",
        "stack_model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('stack', StackingClassifier(\n",
        "        estimators=base_models,\n",
        "        final_estimator=meta_model,\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        passthrough=False\n",
        "    ))\n",
        "])\n",
        "\n",
        "\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# EVALUATION\n",
        "y_pred = stack_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n Stacking Model Accuracy: {acc*100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyHChhHMSLR3",
        "outputId": "c663a0e4-0096-46c7-a31f-4a4257bff08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Training Stacking Model...\n",
            "\n",
            "üéØ Stacking Model Accuracy: 90.24%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92        14\n",
            "           1       0.81      1.00      0.90        13\n",
            "           2       0.92      0.86      0.89        14\n",
            "\n",
            "    accuracy                           0.90        41\n",
            "   macro avg       0.91      0.90      0.90        41\n",
            "weighted avg       0.91      0.90      0.90        41\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridSearchCV + Stack Classifier\n"
      ],
      "metadata": {
        "id": "0oAyiccdX8Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tuning SVM\n",
        "svm_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC(probability=True))\n",
        "])\n",
        "svm_params = {'svm__C': [1, 5, 10], 'svm__kernel': ['linear', 'rbf']}\n",
        "svm_grid = GridSearchCV(svm_pipe, svm_params, cv=3, n_jobs=-1, verbose=1)\n",
        "svm_grid.fit(X_train, y_train)\n",
        "svm_best = svm_grid.best_estimator_\n",
        "print(\" Best SVM:\", svm_grid.best_params_)\n",
        "#Tuning KNN\n",
        "knn_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "knn_params = {'knn__n_neighbors': [3, 5, 7], 'knn__weights': ['uniform', 'distance']}\n",
        "knn_grid = GridSearchCV(knn_pipe, knn_params, cv=3, n_jobs=-1, verbose=1)\n",
        "knn_grid.fit(X_train, y_train)\n",
        "knn_best = knn_grid.best_estimator_\n",
        "print(\" Best KNN:\", knn_grid.best_params_)\n",
        "\n",
        "# Tuning RandomForest\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    {'n_estimators': [100, 150], 'max_depth': [None, 10, 20]},\n",
        "    cv=3, n_jobs=-1, verbose=1\n",
        ")\n",
        "rf_grid.fit(X_train, y_train)\n",
        "rf_best = rf_grid.best_estimator_\n",
        "print(\" Best RF:\", rf_grid.best_params_)\n",
        "\n",
        "# Tuning XGBoost\n",
        "xgb_grid = GridSearchCV(\n",
        "    XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
        "    {'n_estimators': [100, 200], 'max_depth': [3, 5], 'learning_rate': [0.05, 0.1]},\n",
        "    cv=3, n_jobs=-1, verbose=1\n",
        ")\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "xgb_best = xgb_grid.best_estimator_\n",
        "print(\" Best XGB:\", xgb_grid.best_params_)\n",
        "\n",
        "\n",
        "# STACKING CLASSIFIER\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('svm', svm_best),\n",
        "        ('rf', rf_best),\n",
        "        ('knn', knn_best),\n",
        "        ('xgb', xgb_best)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Training final stacking model\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# EVALUATE\n",
        "y_pred = stack_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n Final Stacking Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "# SAVE MODEL\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "joblib.dump(stack_model, \"models/stacking_tuned_model.pkl\")\n",
        "joblib.dump(class_dict, \"models/class_dict.pkl\")\n",
        "print(\"\\n Saved final stacked model with tuned base learners.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6HVhgheTdJw",
        "outputId": "90a821cb-69c3-4f1b-d209-afd886ddaf7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Tuning SVM...\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "‚úÖ Best SVM: {'svm__C': 5, 'svm__kernel': 'rbf'}\n",
            "\n",
            "üîç Tuning KNN...\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "‚úÖ Best KNN: {'knn__n_neighbors': 3, 'knn__weights': 'distance'}\n",
            "\n",
            "üîç Tuning RandomForest...\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "‚úÖ Best RF: {'max_depth': None, 'n_estimators': 100}\n",
            "\n",
            "üöÄ Training final stacking model...\n",
            "\n",
            "üéØ Final Stacking Accuracy: 80.49%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}